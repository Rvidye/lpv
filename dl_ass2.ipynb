{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30937adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae084bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075c6a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21508</th>\n",
       "      <td>One night, barkeeper Randy (Matt Dillon) rescu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33750</th>\n",
       "      <td>Reading the other user comments, the review by...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44874</th>\n",
       "      <td>Foolish hikers go camping in the Utah mountain...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18508</th>\n",
       "      <td>Acolytes presents an interesting mix of origin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36727</th>\n",
       "      <td>...this is, above all else, the typical Crown ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "21508  One night, barkeeper Randy (Matt Dillon) rescu...  positive\n",
       "33750  Reading the other user comments, the review by...  negative\n",
       "44874  Foolish hikers go camping in the Utah mountain...  negative\n",
       "18508  Acolytes presents an interesting mix of origin...  negative\n",
       "36727  ...this is, above all else, the typical Crown ...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641e9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment']=df['sentiment'].map({'negative':0,'positive':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f09377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations=\"./><''!@#$%^&*()_=+,\"\n",
    "\n",
    "def remove_stop_words(x):\n",
    "    word_tokens = word_tokenize(x)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence1 = [w for w in filtered_sentence  if not w.lower() in punctuations]\n",
    "    \n",
    "    return \" \".join(filtered_sentence1).lower()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b34486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a8c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  \n",
    "def lemmatization_corpus(x):\n",
    "    word_tokens = word_tokenize(x)\n",
    "    res=[]\n",
    "    for i in word_tokens:\n",
    "        res.append(lemmatizer.lemmatize(i))\n",
    "    return \" \".join(res)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f532e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(lemmatization_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c979f97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49368</th>\n",
       "      <td>`` mask moving film work many level simplest h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6649</th>\n",
       "      <td>saw movie first time surprised little shocked ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43876</th>\n",
       "      <td>... disney film garbage . br br anyway saw `` ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18026</th>\n",
       "      <td>saw film chance small box fantastic chilling s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14666</th>\n",
       "      <td>think problem show getting respect truly deser...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49368  `` mask moving film work many level simplest h...          1\n",
       "6649   saw movie first time surprised little shocked ...          1\n",
       "43876  ... disney film garbage . br br anyway saw `` ...          0\n",
       "18026  saw film chance small box fantastic chilling s...          1\n",
       "14666  think problem show getting respect truly deser...          1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd507bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 35000\n",
      "Validation size:  10000\n",
      "Test size:  5000\n"
     ]
    }
   ],
   "source": [
    "data_0 = df[df['sentiment'] == 0]\n",
    "data_1 = df[df['sentiment'] == 1]\n",
    "\n",
    "train_size = int(0.7*25000)\n",
    "val_size = int(0.2*25000)\n",
    "\n",
    "data_train = pd.concat((data_0[:train_size], data_1[:train_size]), axis = 0)\n",
    "data_val = pd.concat((data_0[train_size: (train_size + val_size)], data_1[train_size:(train_size + val_size)]), axis = 0)\n",
    "data_test = pd.concat((data_0[(train_size + val_size):], data_1[(train_size + val_size):]), axis = 0)\n",
    "\n",
    "X_train, y_train = list(data_train['review']), np.array(data_train['sentiment'])\n",
    "X_val, y_val = list(data_val['review']), np.array(data_val['sentiment'])\n",
    "X_test, y_test = list(data_test['review']), np.array(data_test['sentiment'])\n",
    "\n",
    "print('Train size:', len(X_train))\n",
    "print('Validation size: ', len(X_val))\n",
    "print('Test size: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a013ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_length = 500\n",
    "trunc_type = 'post'\n",
    "oov_tok = 'OOV'\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "token = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "token.fit_on_texts(X_train)\n",
    "index_word = token.index_word\n",
    "\n",
    "# Convert texts to sequences\n",
    "train_seq = token.texts_to_sequences(X_train)\n",
    "val_seq = token.texts_to_sequences(X_val)\n",
    "test_seq = token.texts_to_sequences(X_test)\n",
    "\n",
    "train_pad = pad_sequences(train_seq, maxlen = max_length, padding = 'post', truncating = trunc_type)\n",
    "val_pad = pad_sequences(val_seq, maxlen = max_length, padding = 'post', truncating = trunc_type)\n",
    "test_pad = pad_sequences(test_seq, maxlen = max_length, padding = 'post', truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49945ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(train_pad))\n",
    "train_pad = train_pad[p]\n",
    "y_train = y_train[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "613f262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, AveragePooling1D, Bidirectional, LSTM, Dense,Flatten,LeakyReLU\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83770ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 64)           640000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 496, 512)          164352    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 253952)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32505984  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,318,657\n",
      "Trainable params: 33,318,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "274/274 [==============================] - ETA: 0s - loss: 0.4004 - accuracy: 0.8077"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length = max_length))\n",
    "model.add(Conv1D(512, kernel_size=5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(1, 'sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "H = model.fit(train_pad, y_train, epochs = 10, batch_size = 128,\n",
    "             validation_data = (val_pad, y_val) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe72c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
